{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST & SNR Example.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pnorridge/think-global-act-local/blob/master/MNIST_%26_SNR_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Be7oNHMeca1g",
        "colab_type": "text"
      },
      "source": [
        "This notebook is a supplement to the paper *Think Global, Act Local: The relationship between DNN generalisation and node-level information preservation.*\n",
        "\n",
        "Abstract: We argue that the (global) generalisation performance of a DNN is related to the information preservation and Signal-to-Noise Ratio of individual nodes. Further, some weight combinations generate better a SNR than others. We demonstrate this by deriving figures-of-merit that can be applied to weight sets and give examples of the correlation between these figures-of-merit with DNN generalisation.\n",
        "\n",
        "This notebook gives an example of the process applied to MNIST classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMl_tooWl_rn",
        "colab_type": "text"
      },
      "source": [
        "The underlying reasoning for considering node-level SNR and SNR-optimising weights can be summarised as:\n",
        "\n",
        "* A DNN will perform best when the available information is used to maximum extent.\n",
        "\n",
        "* Optimal use of information by the entire network depends on maximising the information preservation of individual nodes.\n",
        "\n",
        "* Linsker (R._Linsker 1988) has shown the relationship between the SNR of a node and the maximisation of the information preservation from inputs to output (or, equivalently, the information rate out at the output of the node.) So, if we want to optimise the information flow in the network, we should pay attention to the SNR of the individual nodes.\n",
        "\n",
        "* It is possible to quantify how well a given set of weights optimises the SNR within the context of a given network and training set. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "touBc6i8MBVY",
        "colab_type": "text"
      },
      "source": [
        "# Admin"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFEkxy7Eme41",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Admin\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laR8gH8xWKo-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load in the test data & configure an iterator\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels),(test_images, test_labels) = mnist.load_data()\n",
        "train_images, test_images = tf.cast(train_images, tf.float32) / 255.0, tf.cast(test_images, tf.float32) / 255.0\n",
        "train_labels, test_labels = tf.cast(train_labels, tf.int32), tf.cast(test_labels, tf.int32)\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
        "batched_data = dataset.shuffle(buffer_size=60000).batch(50).repeat()\n",
        "data_iterator = batched_data.make_one_shot_iterator()\n",
        "\n",
        "# Set up a zero vector -- used as reference points when doing correlations\n",
        "x_zero = tf.zeros([10,28,28])\n",
        "corr_test_images = tf.concat([tf.cast(test_images,tf.float32),x_zero], axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULy3K0TlY2Sy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Essential model-independent helper functions\n",
        "def weight_variable(shape):\n",
        "  initial = tf.Variable(tf.truncated_normal(shape, stddev=0.1))\n",
        "  return initial\n",
        "\n",
        "def bias_variable(shape):\n",
        "  initial = tf.constant(0.1, shape=shape)\n",
        "  return tf.Variable(initial)\n",
        "    \n",
        "def measure_accuracy(labels, logits):\n",
        "  correct_prediction = tf.equal(tf.cast(labels, tf.int64), tf.argmax(logits, axis=1))\n",
        "  return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "def cross_entropy(x, y):\n",
        "  return tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=x))\n",
        "\n",
        "\n",
        "def dot_layer(f, wts):  \n",
        "  def layer(x,p): \n",
        "    return tf.matmul(f(x,p),wts)\n",
        "  \n",
        "  return layer\n",
        "\n",
        "\n",
        "def relu_layer(f, bs):\n",
        "  def layer(x,p):\n",
        "    return tf.nn.dropout(tf.nn.relu(f(x,p)+bs),p)\n",
        "  \n",
        "  return layer\n",
        "\n",
        "\n",
        "def masked_mean(measure, weight = None):\n",
        "  if weight is None:\n",
        "    weight = tf.ones(measure.shape)\n",
        "  \n",
        "  mean = tf.reduce_mean(tf.boolean_mask(measure,tf.logical_not(tf.is_nan(measure))))\n",
        "  return mean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttueUSkGrQRQ",
        "colab_type": "text"
      },
      "source": [
        "# Building blocks for figures-of-merit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnyGZQ6pZ77c",
        "colab_type": "text"
      },
      "source": [
        "We start by defining a modified covariance function. This calculates the covariance between a node input and the output -- and scales by the non-zero rate of the input.\n",
        "\n",
        "\\begin{equation}\n",
        " c_{i}= \\frac{\\text{cov} \\left( x_i,\\sum w_{ij} x_j\\right)}{a_i} = \\frac{\\text{cov} \\left( x_i,r_i\\right)}{a_i} \n",
        "\\end{equation}\n",
        "\n",
        "$ a_i = p(x_i > 0) $\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGgPcrxGZAoz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def relu_covariance_batch(x,r):\n",
        "  # This calculates the covariance, including the scaling factor appropriate for ReLU-like inputs.\n",
        "  # Implemented with batching due to memory issues with large inputs.\n",
        "\n",
        "  batch_sz = 50\n",
        "  \n",
        "  leng = tf.cast(x.shape[0],tf.float32)\n",
        "\n",
        "  m = int(leng/batch_sz-1)\n",
        "\n",
        "  # Normalise the inputs. This is a quick and dirty way to get the noise variances to approximately the same magnitude.\n",
        "  max_count = tf.cast(x.shape[0], tf.float32)\n",
        "  x = x/(tf.reduce_max(x, axis = 0)+0.001)\n",
        "  \n",
        "  xx = tf.cast(tf.expand_dims(x,-1),tf.float32)\n",
        "  rr = tf.expand_dims(r,1)\n",
        "  \n",
        "  # Shift inputs to be zero mean\n",
        "  xxm = tf.reduce_sum(xx[batch_sz*m:], axis = 0)\n",
        "  rrm = tf.reduce_sum(rr[batch_sz*m:], axis = 0)\n",
        "  for k in range(0,m):\n",
        "    xxm = xxm + tf.reduce_sum(xx[batch_sz*k:batch_sz*(k+1)], axis = 0)\n",
        "    rrm = rrm + tf.reduce_sum(rr[batch_sz*k:batch_sz*(k+1)], axis = 0)\n",
        "  \n",
        "  xx = xx - xxm/leng #(x_count+0.0001) # lazily ensure no 1/0\n",
        "  rr = rr - rrm/tf.cast(rr.shape[0],tf.float32)\n",
        "  \n",
        "  # Calculate the variance\n",
        "  xr = tf.reduce_sum(xx[batch_sz*m:,:,:]*rr[batch_sz*m:,:,:], axis = 0)\n",
        "  for k in range(0,m):\n",
        "    xr = xr + tf.reduce_sum(xx[batch_sz*k:batch_sz*(k+1),:,:]*rr[batch_sz*k:batch_sz*(k+1),:,:], axis = 0)\n",
        "\n",
        "  # Scaling for ReLU inputs \n",
        "  x_count = tf.reduce_sum(tf.cast(xx>0, tf.float32), axis = 0) \n",
        "  xr = xr/(x_count+0.0001) # lazily ensure no 1/0\n",
        "\n",
        "  return xr\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-7nAYBjacxJ",
        "colab_type": "text"
      },
      "source": [
        "To derive the SNR Optimiality expression, we start with the usual (signal processing) definition of SNR applied to node $i$ in layer $m$\n",
        "\n",
        "\\begin{equation}\n",
        "SNR_i^{(m)} =\\frac{\\text{var} \\left(\\sum_{j}w_{ij}s_{j} \\right)}{ \\sum_{j}\\text{var} \\left( w_{ij}n_{j} \\right) } \n",
        "\\end{equation}\n",
        "\n",
        "where $s_j$ are the signal and $n_j$ are the noise components. At this point we leave open how to partition inputs into these components. In all cases, variance and covariances are calculated over a batch. We look for weights that maximise the expression.  \n",
        "\n",
        "\\begin{equation}\n",
        "w_{ij}= k_i .  \\frac{\\text{cov} \\left( s_j, \\sum w_{ij} s_i \\right) }{\\text{var} \\left( n_j \\right)}  \n",
        "\\end{equation}\n",
        "\n",
        "Where  $ k_i $  is a constant independent of  \\( j \\)  and we have implicitly assumed that  $ \\text{var} \\left( n_{j} \\right)\\neq 0 $ .\\par\n",
        "\n",
        "To allow us to use this, we make two pragmatic assumptions. \n",
        "The first is that the after training, the signal dominates the node output, so that \n",
        "\\begin{equation}\n",
        "\\text{cov} \\left( s_i, \\sum w_{ij} s_j \\right) \\approx \\text{cov} \\left( x_j, \\sum w_{ij} x_i \\right)\n",
        "\\end{equation}\n",
        "\n",
        "The second is that the noise has identical variance for each non-zero input sample and zero variance otherwise. With these assumptions, we make the approximation\n",
        "\\begin{equation}\n",
        " \\text{var} \\left( n_{j} \\right) \\approx a_j ~ \\text{var}(n)\n",
        "\\end{equation}\n",
        "\n",
        "where $\\text{var}(n)$ is the common noise variance of all samples arriving at node $i$ and $a_i$ is the rate of activations of node $j$ of layer $(m-1)$. \n",
        "\n",
        "Using these assumptions, we update the expression for optimal weights to\n",
        "\\begin{equation}\n",
        "w_{ij} = k_i' .  \\frac{\\text{cov} \\left( x_i, \\sum w_{ij} x_j \\right) }{ a_j }  \n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "We do not expect that nodes will generally meet this condition, but we would like to assess how close a given weight configuration is to 'optimal'. We can measure this by treating $w_{ij}$ and $\\frac{\\text{cov} \\left( x_i, \\sum w_{ij} x_j \\right) }{\\left( a_j \\right)}$ as vectors and calculating the inner product between them\\footnote{At this point, we note that there is a close relationship between this condition and PCA. When $a_j = 1 \\ \\forall j$, the weight will be optimal if it is an eigenvector of the covariance matrix.}.\n",
        "\n",
        "\\begin{equation}\n",
        "S_i =\\frac{ \\sum _{j}w_{ij}c_{j}}{ \\vert \\vec{w_{i}} \\vert  \\vert \\vec{c} \\vert } \n",
        "\\end{equation}\n",
        "\n",
        "where $\\vec{c}$ is as defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHk0wiQyafk0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def SNROptimality_batch(x, r, W):\n",
        "  \n",
        "    xR = relu_covariance_batch(x,r)\n",
        "    \n",
        "    nm = (tf.norm(xR,axis=0)+0.0001)\n",
        "    c_hat = xR/nm\n",
        "\n",
        "    \n",
        "    count = tf.reduce_sum(tf.cast(c_hat*W<0, tf.float32))\n",
        "    numer = tf.reduce_sum(c_hat*W, axis = 0)\n",
        "  \n",
        "    norm_wtv = tf.norm(W, axis=0)\n",
        "\n",
        "    denom = norm_wtv\n",
        "\n",
        "    return (numer/denom, count)\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Kua4Jnwaj5D",
        "colab_type": "text"
      },
      "source": [
        "To measure SNR improvement, we compare the SNR Optimiality for a given weight vector with the case where we simply selected the most representative input and passed that through transparently. That is, we calculating $S_i$ using only the input that is best correlated with the output of the node. We fix $c_j$ and then use $w_{ij}'$ defined by\n",
        "\\begin{equation}\n",
        "    w_{ij}' =  \\begin{cases}\n",
        "                1 \\ \\ \\  \\text{if }  j=\\text{argmax}_k \\left( c_k \\right)   \\\\\n",
        "               0 \\ \\ \\  \\text{otherwise}\n",
        "            \\end{cases}\n",
        "\\end{equation}\n",
        "\n",
        "For this choice,\n",
        "\\begin{equation} \n",
        "S_i' = \\frac{ \\sum _{j}^{}w_{ij}'c_{j}}{ \\vert \\vec{w'_i} \\vert  \\vert \\vec{c} \\vert }  =  \\frac{\\max_{i} \\left( c_{i} \\right) }{ \\vert \\vec{c} \\vert } \n",
        "\\end{equation}\n",
        "\n",
        "The ratio of the $S_i$ and $S_i'$ becomes\n",
        "\\begin{equation}\n",
        "F=\\frac{ \\sum _{j}^{}w_{ij}c_{j}}{ \\vert \\vec{w_i} \\vert \\max_{i} \\left( c_{i} \\right) } \n",
        "\\end{equation}\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoUKmZyYlDkL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def SignalFactor_batch(x, r, W):\n",
        "  \n",
        "    xR = relu_covariance_batch(x,r)\n",
        "    \n",
        "    nm = (tf.reduce_max(tf.abs(xR),axis=0,keepdims=True))\n",
        "    c_hat = xR/nm\n",
        "\n",
        "    \n",
        "    count = tf.reduce_sum(tf.cast(c_hat*W<0, tf.float32))\n",
        "    numer = tf.reduce_sum(c_hat*W, axis = 0)\n",
        "  \n",
        "    norm_wtv = tf.norm(W, axis=0)\n",
        "\n",
        "    denom = norm_wtv\n",
        "\n",
        "    return (numer/denom, count)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKmVlbl0qcwP",
        "colab_type": "text"
      },
      "source": [
        "The following will be used later to aid with visualisation of the SNR optimality"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QC-OAKGSqcRO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Optimality_Plot(x, r, W, node_list):\n",
        "  \n",
        "    xR = relu_covariance_batch(x,r)\n",
        "        \n",
        "    W1 = W[:,node_list]\n",
        " \n",
        "    c_hat = xR/(tf.norm(xR, axis=0))\n",
        "\n",
        "    norm_wtv = tf.norm(W1, axis=0)\n",
        "\n",
        "    normed_wt = W1/norm_wtv\n",
        "\n",
        "    fig,ax = plt.subplots(figsize=(5,5))\n",
        "\n",
        "    ax.plot(normed_wt,c_hat[:,node_list],'.')\n",
        "    ax.fill_between([0, tf.reduce_max(normed_wt).numpy()], 0,tf.reduce_max(c_hat[:,node_list]).numpy(),alpha=0.2, color='#1F98D0')  \n",
        "    ax.fill_between([tf.reduce_min(normed_wt).numpy(), 0], tf.reduce_min(c_hat[:,node_list]).numpy(), 0, alpha=0.2, color='#1F98D0')  \n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07u-Yt6es3Ri",
        "colab_type": "text"
      },
      "source": [
        "<h1>Model definition </h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GclIlK14Sz4",
        "colab_type": "text"
      },
      "source": [
        "Define our base class. The costruction is realtively low-level to allow easy access to all the information needed for the figures-of-merit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2iOnaKaXnsN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class trainable_function:\n",
        "\n",
        "  def __init__(self, withWeightsFrom = None):\n",
        "    self.W = []\n",
        "    self.b = []\n",
        "    # Set up the weights\n",
        "    if withWeightsFrom == None:\n",
        "      self.W.append(weight_variable([784, 1024]))\n",
        "      self.b.append(bias_variable([1024]))\n",
        "    \n",
        "      self.W.append(weight_variable([1024, 1000]))\n",
        "      self.b.append(bias_variable([1000]))\n",
        "\n",
        "      self.W.append(weight_variable([1000, 10]))\n",
        "      self.b.append(bias_variable([10]))\n",
        "      \n",
        "    else:\n",
        "      for k in range(len(withWeightsFrom.W)):\n",
        "        self.W.append(tf.Variable(withWeightsFrom.W[k]))\n",
        "        self.b.append(tf.Variable(withWeightsFrom.b[k]))\n",
        "\n",
        "    self.variables = [self.W[2], self.b[2], self.W[1], self.b[1], self.W[0], self.b[0]]\n",
        "    \n",
        "    # Build the network with lambda functions.\n",
        "    self.layer_dot = [lambda x, p: x]\n",
        "    self.layer_out = [lambda x, p: tf.reshape(x,[-1,784])]\n",
        "    \n",
        "    for kk in range(1,len(self.W)):\n",
        "      self.layer_dot.append(dot_layer(self.layer_out[kk-1], self.W[kk-1])) \n",
        "      self.layer_out.append(relu_layer(self.layer_dot[kk], self.b[kk-1]))\n",
        "\n",
        "    self.layer_dot.append(dot_layer(self.layer_out[-1], self.W[-1])) \n",
        "\n",
        "    self.optimiser = tf.train.GradientDescentOptimizer(0.1)\n",
        "\n",
        "\n",
        "  def classify(self, x, p = 1.):\n",
        "    return self.layer_dot[-1](x,p)\n",
        "\n",
        "\n",
        "  def train(self, x, y):\n",
        "    null\n",
        "\n",
        "  # Calculate F for one layer\n",
        "  def SignalFactor(self, x, layer):\n",
        "    x_int = self.layer_out[layer-1](x,1.)\n",
        "    r = tf.matmul(x_int, self.W[layer-1]) \n",
        "\n",
        "    return SignalFactor_batch(x_int, r, self.W[layer-1])\n",
        "\n",
        "\n",
        "  # Calculate mean(F) for each layer\n",
        "  def SF_summary(self, x):\n",
        "    res = []\n",
        "    for k in range(len(self.layer_out)):\n",
        "      (measure, count)  = self.SignalFactor(corr_test_images, k+1)\n",
        "      res.append(masked_mean(measure))\n",
        "    return res\n",
        "\n",
        "\n",
        "  # Calculate S for one layer\n",
        "  def SNROptimality(self, x, layer):\n",
        "    x_int = self.layer_out[layer-1](x,1.)\n",
        "    r = tf.matmul(x_int, self.W[layer-1]) \n",
        "\n",
        "    return SNROptimality_batch(x_int, r, self.W[layer-1])\n",
        "  \n",
        "  # Calculate mean(S) for each layer\n",
        "  def SNR_summary(self, x):\n",
        "    res = []\n",
        "    for k in range(len(self.layer_out)):\n",
        "      (measure, count)  = self.SNROptimality(corr_test_images, k+1)\n",
        "      res.append(masked_mean(measure))\n",
        "    return res\n",
        "\n",
        "\n",
        "  def Optimality_plot(self, x, layer, nodes):\n",
        "    x_int = self.layer_out[layer-1](x,1.)\n",
        "    r = tf.matmul(x_int, self.W[layer-1]) \n",
        "    Optimality_Plot(x_int, r, self.W[layer-1], nodes)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5kK6EBK4Ky0",
        "colab_type": "text"
      },
      "source": [
        "Sub-classes representing different training methods/regularisation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXpADLpe32q9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# Basic SGD with no regulsatisation\n",
        "class trainable_function_with_noreg(trainable_function):\n",
        "\n",
        "  def calc_grad(self, inputs, targets):\n",
        "    with tf.GradientTape() as tape:\n",
        "      loss_value = cross_entropy(self.classify(inputs, p = 1.), targets)\n",
        "    return tape.gradient(loss_value, self.variables)\n",
        "\n",
        "  def train(self, x,y):\n",
        "    grads = self.calc_grad(x, y)\n",
        "    self.optimiser.apply_gradients(zip(grads, self.variables))\n",
        "\n",
        "# Basic SGD with dropout\n",
        "class trainable_function_with_dropout(trainable_function):\n",
        "\n",
        "  def calc_grad(self, inputs, targets):\n",
        "    with tf.GradientTape() as tape:\n",
        "      loss_value = cross_entropy(self.classify(inputs, p = 0.5), targets)\n",
        "    return tape.gradient(loss_value, self.variables)\n",
        "\n",
        "  def train(self, x,y):\n",
        "    grads = self.calc_grad(x, y)\n",
        "    self.optimiser.apply_gradients(zip(grads, self.variables))\n",
        "    \n",
        "\n",
        "# Basic SGD with L2 regulsatisation\n",
        "class trainable_function_with_L2(trainable_function):\n",
        "\n",
        "  def calc_grad(self, inputs, targets):\n",
        "    with tf.GradientTape() as tape:\n",
        "      loss_value = cross_entropy(self.classify(inputs, p = 1.), targets) \\\n",
        "                                 +0.0001*tf.nn.l2_loss(self.W[0])\\\n",
        "                                 +0.0001*tf.nn.l2_loss(self.W[1])\\\n",
        "                                 +0.0001*tf.nn.l2_loss(self.W[2])\n",
        "                                 \n",
        "    return tape.gradient(loss_value, self.variables)\n",
        "\n",
        "  def train(self, x,y):\n",
        "    grads = self.calc_grad(x, y)\n",
        "    self.optimiser.apply_gradients(zip(grads, self.variables))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlThZLN1svDs",
        "colab_type": "text"
      },
      "source": [
        "<h1>Training & evaluation</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDOPNrG_35a_",
        "colab_type": "text"
      },
      "source": [
        "Set up three models for comparison. We use identical starting weights and training data, to ensure that there is a comparison between training methods, not simply the initialisation and data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYDLOui60Kmg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "control = trainable_function_with_noreg()\n",
        "model_n = trainable_function_with_noreg(control)\n",
        "model_d = trainable_function_with_dropout(control)\n",
        "model_l2 = trainable_function_with_L2(control)\n",
        "\n",
        "model_list = [model_n, model_d, model_l2]\n",
        "\n",
        "test_acc = {lst: [] for lst in model_list} \n",
        "SNR = {lst: [] for lst in model_list} \n",
        "SF = {lst: [] for lst in model_list} \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahbd233Tk7gF",
        "colab_type": "text"
      },
      "source": [
        "Training with regular calculations of the figures-of-merit. A good time for a coffee.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ef4b1fb3-8c65-412e-9cd4-c47dbbc5a9c3",
        "id": "MetEhGoi_91A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        }
      },
      "source": [
        "# Main loop\n",
        "for i in range(100001):\n",
        "\n",
        "  # Get training data\n",
        "  (x,y) = data_iterator.get_next()\n",
        "\n",
        "  # Loop over models\n",
        "  for model in model_list:\n",
        "\n",
        "      # Instrumentation\n",
        "\n",
        "      if i % 200 == 0:\n",
        "        test_accuracy  = measure_accuracy(labels = test_labels, logits = model.classify(test_images))\n",
        "        test_acc[model].append(test_accuracy)\n",
        "\n",
        "\n",
        "      if i % 2000 == 0:\n",
        "        SF[model].append(model.SF_summary(corr_test_images))\n",
        "        SNR[model].append(model.SNR_summary(corr_test_images))\n",
        "        \n",
        "\n",
        "      if i % 20000 == 0:\n",
        "        print('\\n Step %d' % i)\n",
        "        print(type(model).__name__)\n",
        "        print('Test accuracy %g' % test_accuracy)\n",
        "        print('L1: %g' % SNR[model][-1][0])\n",
        "        print('L2: %g' % SNR[model][-1][1])\n",
        "        print('L3: %g' % SNR[model][-1][2])\n",
        "\n",
        "\n",
        "      # Actual training          \n",
        "      model.train(x, y)\n",
        "\n",
        "  if i % 20000 == 0:\n",
        "    print('------\\n')\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Step 0\n",
            "trainable_function_with_noreg\n",
            "Test accuracy 0.1003\n",
            "L1: 0.182793\n",
            "L2: 0.157043\n",
            "L3: 0.186966\n",
            "\n",
            " Step 0\n",
            "trainable_function_with_dropout\n",
            "Test accuracy 0.1003\n",
            "L1: 0.182793\n",
            "L2: 0.157043\n",
            "L3: 0.186966\n",
            "\n",
            " Step 0\n",
            "trainable_function_with_L2\n",
            "Test accuracy 0.1003\n",
            "L1: 0.182793\n",
            "L2: 0.157043\n",
            "L3: 0.186966\n",
            "------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3fyuw2pyA5F",
        "colab_type": "text"
      },
      "source": [
        "# Test accuracy for each model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WINtJPKuptjG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for m in model_list:\n",
        "  plt.plot(test_acc[m])\n",
        "plt.ylim([0.97,0.99])\n",
        "plt.ylabel('test accuracy')\n",
        "plt.xlabel('iterations (200x)')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0G2OR1MyGMn",
        "colab_type": "text"
      },
      "source": [
        "# SNR optimality\n",
        "\n",
        "Note that the SNR optimality is correlated with performance after convergence. Interestingly, for dropout the SNR optimality indicates the enhanced performance before this is seen in the test accuracy results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kx8DPx0O9lI3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, a = plt.subplots(1,3,figsize=(15, 5))\n",
        "for m in model_list:\n",
        "  a[0].plot([SNR[m][a][0] for a in range(len(SNR[m]))])\n",
        "  a[1].plot([SNR[m][a][1] for a in range(len(SNR[m]))])\n",
        "  a[2].plot([SNR[m][a][2] for a in range(len(SNR[m]))])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LD78JeA-yb-9",
        "colab_type": "text"
      },
      "source": [
        "# Signal Figure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCZf_HW5Ehvz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, a = plt.subplots(1,3,figsize=(15, 5))\n",
        "for m in model_list:\n",
        "  a[0].plot([SF[m][a][0] for a in range(len(SF[m]))])\n",
        "  a[1].plot([SF[m][a][1] for a in range(len(SF[m]))])\n",
        "  a[2].plot([SF[m][a][2] for a in range(len(SF[m]))])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzDLh_XCx2n4",
        "colab_type": "text"
      },
      "source": [
        "# Scatter plots of $W$ vs. $c$\n",
        "\n",
        "Heere, we plot $W$ against $c$ for one node. This gives a more intuitive & visual sense of what it means to have good SNR optimality. The points become increasingly close to the line $W=c$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_EUNVcX7wsg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for m in control+model_list:\n",
        "  ind = tf.argmax(m.SNROptimality(corr_test_images,3)[0])\n",
        "  m.Optimality_plot(corr_test_images,3,ind)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GECIkBux33fr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m.SNROptimality(corr_test_images,2)[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIWboaMT4RdE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ind"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8-MWmG7M4xw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}