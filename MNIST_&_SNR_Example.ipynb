{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST & SNR Example.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "touBc6i8MBVY"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c2ff9badeef341a49507a37a5cc26cd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5d46b40265814c98851f44cd7391840a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_87ac59a28368409c8e35f8abc745e55a",
              "IPY_MODEL_2e90f2dd257d49a3ac831b1e47938036"
            ]
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pnorridge/think-global-act-local/blob/master/MNIST_%26_SNR_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yB3rMNC7h0BB",
        "colab_type": "text"
      },
      "source": [
        "# Think Global, Act Local: MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Be7oNHMeca1g",
        "colab_type": "text"
      },
      "source": [
        "This notebook is a supplement to the paper *Think Global, Act Local: The relationship between DNN generalisation and node-level information preservation.*\n",
        "\n",
        "Abstract: The reasons behind good DNN generalisation remain an open question. In this paper we explore the problem by looking at the Signal-to-Noise Ratio of nodes in the network. Starting from information theory principles, it is possible to derive an expression for the SNR of a DNN node output. Using this expression we construct figures-of-merit that quantify how well the weights of a node optimise SNR (or, equivalently, information rate). Applying these figures-of-merit, we give examples indicating that weight sets that promote good SNR performance also exhibit good generalisation.\n",
        "\n",
        "This notebook gives an example of the process applied to MNIST classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMl_tooWl_rn",
        "colab_type": "text"
      },
      "source": [
        "The underlying reasoning for considering node-level SNR and SNR-optimising weights can be summarised as:\n",
        "\n",
        "* A DNN will perform best when the available information is used to maximum extent.\n",
        "\n",
        "* Optimal use of information by the entire network depends on maximising the information preservation of individual nodes.\n",
        "\n",
        "* With some assumptions, it is possible to derive a relationship between the output SNR of a node and the maximisation of the information preservation from inputs to output (characterised by the mutual information or the information rate at the output of the node). So, if we want to optimise the information flow in the network, we should pay attention to the SNR of the individual nodes.\n",
        "\n",
        "* It is possible to quantify how well a given set of weights optimises the SNR within the context of a given network and training set. Even though we cannot define a priori the signal that a node should generate, we can construct formulae that allow the SNR quality of a post-training weight set to be assessed. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "touBc6i8MBVY",
        "colab_type": "text"
      },
      "source": [
        "# Admin\n",
        "\n",
        "Import packages and test data. Plus define some basic functions that will be useful."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFEkxy7Eme41",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Admin\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laR8gH8xWKo-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190,
          "referenced_widgets": [
            "c2ff9badeef341a49507a37a5cc26cd6"
          ]
        },
        "outputId": "bcdb94bf-2741-4352-e638-9349bf81d761"
      },
      "source": [
        "# Load in the test data & configure an iterator\n",
        "train_data, test_data = tfds.load(\"mnist\", split=[tfds.Split.TRAIN, tfds.Split.TEST])\n",
        "#train_data, test_data = data['train'], data['test']\n",
        "\n",
        "def data_preprocess(record):\n",
        "  img = tf.image.convert_image_dtype(record['image'],tf.float32)\n",
        "  lab = tf.cast(record['label'],tf.int32)\n",
        "  return dict(image = img, label = lab)\n",
        "\n",
        "train_data = train_data.map(data_preprocess)\n",
        "test_data = test_data.map(data_preprocess)\n",
        "\n",
        "batched_data = train_data.shuffle(buffer_size=60000).batch(60).repeat(1000).prefetch(10)\n",
        "data_iterator = iter(batched_data)\n",
        "\n",
        "# For this example, we want to use all the test data for the SNR calculations. \n",
        "# A large data set ensures that the covariances are representative.\n",
        "all_test_data = iter(test_data.batch(10000)).get_next()\n",
        "\n",
        "\n",
        "# Set up a zero vector -- used as reference points when doing correlations\n",
        "x_zero = tf.zeros([10,28,28])\n",
        "test_images = all_test_data['image']\n",
        "test_labels = all_test_data['label']\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Dataset mnist is hosted on GCS. It will automatically be downloaded to your\n",
            "local data directory. If you'd instead prefer to read directly from our public\n",
            "GCS bucket (recommended if you're running on GCP), you can instead set\n",
            "data_dir=gs://tfds-data/datasets.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset mnist (11.06 MiB) to /root/tensorflow_datasets/mnist/3.0.0...\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c2ff9badeef341a49507a37a5cc26cd6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Dl Completed...', max=4, style=ProgressStyle(description_widtâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1mDataset mnist downloaded and prepared to /root/tensorflow_datasets/mnist/3.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULy3K0TlY2Sy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Essential model-independent helper functions\n",
        "def weight_variable(shape):\n",
        "  initial = tf.Variable(tf.truncated_normal(shape, stddev=0.1))\n",
        "  return initial\n",
        "\n",
        "def bias_variable(shape):\n",
        "  initial = tf.constant(0.1, shape=shape)\n",
        "  return tf.Variable(initial)\n",
        "    \n",
        "def measure_accuracy(labels, logits):\n",
        "  correct_prediction = tf.equal(tf.cast(labels, tf.int64), tf.argmax(logits, axis=1))\n",
        "  return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "def cross_entropy(x, y):\n",
        "  return tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=x))\n",
        "\n",
        "# Layer function generators. \n",
        "# Note that we split the layers into the weight multiplcation and the \n",
        "# offset+relu parts. This is convenient for the SNR calculations.\n",
        "\n",
        "def dot_layer(f, wts):  \n",
        "  def layer(x,p): \n",
        "    return tf.matmul(f(x,p),wts)\n",
        "  \n",
        "  return layer\n",
        "\n",
        "def relu_layer(f, bs):\n",
        "  def layer(x,p):\n",
        "    return tf.nn.dropout(tf.nn.relu(f(x,p)+bs),p)\n",
        "  \n",
        "  return layer\n",
        "\n",
        "# Mean calculation with removal of NaN terms, if any.\n",
        "def masked_mean(measure, weight = None):\n",
        "  if weight is None:\n",
        "    weight = tf.ones(measure.shape)\n",
        "  \n",
        "  mean = tf.reduce_mean(tf.boolean_mask(measure,tf.logical_not(tf.is_nan(measure))))\n",
        "  return mean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttueUSkGrQRQ",
        "colab_type": "text"
      },
      "source": [
        "# Building blocks for figures-of-merit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnyGZQ6pZ77c",
        "colab_type": "text"
      },
      "source": [
        "We start by defining a modified covariance function. This calculates the covariance between a node input and the output -- and scales by the non-zero rate of the input.\n",
        "\n",
        "\\begin{equation}\n",
        " c_{i}= \\frac{\\text{cov} \\left( x_i,\\sum w_{ij} x_j\\right)}{a_i} = \\frac{\\text{cov} \\left( x_i,r_i\\right)}{a_i} \n",
        "\\end{equation}\n",
        "\n",
        "$ a_i = p(x_i > 0) $\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGgPcrxGZAoz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def relu_covariance_batch(x,r):\n",
        "  # This calculates the covariance, including the scaling factor appropriate for ReLU-like inputs.\n",
        "  # Implemented with batching due to memory issues with large inputs.\n",
        "\n",
        "  batch_sz = 50\n",
        "  \n",
        "  leng = tf.cast(x.shape[0],tf.float32)\n",
        "\n",
        "  m = int(leng/batch_sz-1)\n",
        "\n",
        "  # Normalise the inputs. This is a quick and dirty way to get the noise variances to approximately the same magnitude.\n",
        "  max_count = tf.cast(x.shape[0], tf.float32)\n",
        "  x = x/(tf.reduce_max(x, axis = 0)+0.001)\n",
        "  \n",
        "  xx = tf.cast(tf.expand_dims(x,-1),tf.float32)\n",
        "  rr = tf.expand_dims(r,1)\n",
        "  \n",
        "  # Shift inputs to be zero mean\n",
        "  xxm = tf.reduce_sum(xx[batch_sz*m:], axis = 0)\n",
        "  rrm = tf.reduce_sum(rr[batch_sz*m:], axis = 0)\n",
        "  for k in range(0,m):\n",
        "    xxm = xxm + tf.reduce_sum(xx[batch_sz*k:batch_sz*(k+1)], axis = 0)\n",
        "    rrm = rrm + tf.reduce_sum(rr[batch_sz*k:batch_sz*(k+1)], axis = 0)\n",
        "  \n",
        "  rr = rr - rrm/tf.cast(rr.shape[0],tf.float32)\n",
        "  # Calculate the variance\n",
        "  xr = tf.reduce_sum(xx[batch_sz*m:,:,:]*rr[batch_sz*m:,:,:], axis = 0)\n",
        "  for k in range(0,m):\n",
        "    xr = xr + tf.reduce_sum(xx[batch_sz*k:batch_sz*(k+1),:,:]*rr[batch_sz*k:batch_sz*(k+1),:,:], axis = 0)\n",
        "\n",
        "\n",
        "  # Scaling for ReLU inputs \n",
        "  x_count = tf.reduce_sum(tf.cast(xx>0, tf.float32), axis = 0) \n",
        "  xr = xr/(x_count+0.0001) # lazily ensure no 1/0\n",
        "\n",
        "  return xr\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-7nAYBjacxJ",
        "colab_type": "text"
      },
      "source": [
        "# SNR Optimality \n",
        "\n",
        "To derive the SNR Optimiality expression, we start with the usual (signal processing) definition of SNR applied to node $i$ in layer $m$\n",
        "\n",
        "\\begin{equation}\n",
        "SNR_i^{(m)} =\\frac{\\text{var} \\left(\\sum_{j}w_{ij}s_{j} \\right)}{ \\sum_{j}\\text{var} \\left( w_{ij}n_{j} \\right) } \n",
        "\\end{equation}\n",
        "\n",
        "where $s_j$ are the signal and $n_j$ are the noise components. We find weights that maximise the expression.  \n",
        "\n",
        "\\begin{equation}\n",
        "w_{ij}= k_i .  \\frac{\\text{cov} \\left( s_j, \\sum w_{ij} s_i \\right) }{\\text{var} \\left( n_j \\right)}  \n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "To allow us to use this, we make two pragmatic assumptions. \n",
        "The first is that the after training, the signal dominates the node output, so that \n",
        "\\begin{equation}\n",
        "\\text{cov} \\left( s_i, \\sum w_{ij} s_j \\right) \\approx \\text{cov} \\left( x_j, \\sum w_{ij} x_i \\right)\n",
        "\\end{equation}\n",
        "\n",
        "The second is that the noise has identical variance for each non-zero input sample and zero variance otherwise. So,\n",
        "\\begin{equation}\n",
        " \\text{var} \\left( n_{j} \\right) \\approx a_j ~ \\text{var}(n)\n",
        "\\end{equation}\n",
        "\n",
        "where $\\text{var}(n)$ is the noise variance of all non-zero samples arriving at node $i$ and $a_i$ is the rate of activations of node $j$ of layer $(m-1)$. \n",
        "\n",
        "Using these assumptions, we update the expression for optimal weights to\n",
        "\\begin{equation}\n",
        "w_{ij} = k_i' .  \\frac{\\text{cov} \\left( x_i, \\sum w_{ij} x_j \\right) }{ a_j }  \n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "We do not expect that nodes will generally meet this condition, but we would like to assess how close a given weight configuration is to 'optimal'. We can measure this by treating $w_{ij}$ and $\\frac{\\text{cov} \\left( x_i, \\sum w_{ij} x_j \\right) }{\\left( a_j \\right)}$ as vectors and calculating the inner product between them.\n",
        "\n",
        "\\begin{equation}\n",
        "S_i =\\frac{ \\sum _{j}w_{ij}c_{j}}{ \\| \\vec{w_{i}} \\|  \\| \\vec{c} \\| } \n",
        "\\end{equation}\n",
        "\n",
        "where $\\vec{c}$ is as defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHk0wiQyafk0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def SNROptimality_batch(x, r, W):\n",
        "  \n",
        "    xR = relu_covariance_batch(x,r)\n",
        "    \n",
        "    nm = (tf.norm(xR,axis=0)+0.0001)\n",
        "    c_hat = xR/nm\n",
        "\n",
        "    \n",
        "    count = tf.reduce_sum(tf.cast(c_hat*W<0, tf.float32))\n",
        "    numer = tf.reduce_sum(c_hat*W, axis = 0)\n",
        "  \n",
        "    norm_wtv = tf.norm(W, axis=0)\n",
        "\n",
        "    denom = norm_wtv\n",
        "\n",
        "    return (numer/denom, count)\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKmVlbl0qcwP",
        "colab_type": "text"
      },
      "source": [
        "The following will be used later to aid with visualisation of the SNR optimality"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QC-OAKGSqcRO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Optimality_Plot(x, r, W, node_list):\n",
        "  \n",
        "    xR = relu_covariance_batch(x,r)\n",
        "        \n",
        "    W1 = W[:,node_list]\n",
        " \n",
        "    c_hat = xR/(tf.norm(xR, axis=0))\n",
        "\n",
        "    norm_wtv = tf.norm(W1, axis=0)\n",
        "\n",
        "    normed_wt = W1/norm_wtv\n",
        "\n",
        "    fig,ax = plt.subplots(figsize=(5,5))\n",
        "\n",
        "    ax.plot(normed_wt,c_hat[:,node_list],'.')\n",
        "    ax.fill_between([0, tf.reduce_max(normed_wt).numpy()*1.1], 0,tf.reduce_max(c_hat[:,node_list]).numpy()*1.1,alpha=0.2, color='#1F98D0')  \n",
        "    ax.fill_between([tf.reduce_min(normed_wt).numpy()*1.1, 0], tf.reduce_min(c_hat[:,node_list]).numpy()*1.1, 0, alpha=0.2, color='#1F98D0')  \n",
        "\n",
        "    return ax"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Kua4Jnwaj5D",
        "colab_type": "text"
      },
      "source": [
        "# Signal Factor\n",
        "To measure SNR improvement, we compare the SNR Optimiality for a given weight vector with the case where we simply selected the most representative input and passed that through transparently. That is, we calculating $S_i$ using only the input that is best correlated with the output of the node. We fix $c_j$ and then use $w_{ij}'$ defined by\n",
        "\\begin{equation}\n",
        "    w_{ij}' =  \\begin{cases}\n",
        "                1 \\ \\ \\  \\text{if }  j=\\text{argmax}_k \\left( c_k \\right)   \\\\\n",
        "               0 \\ \\ \\  \\text{otherwise}\n",
        "            \\end{cases}\n",
        "\\end{equation}\n",
        "\n",
        "For this choice,\n",
        "\\begin{equation} \n",
        "S_i' = \\frac{ \\sum _{j}^{}w_{ij}'c_{j}}{ \\| \\vec{w'_i} \\|  \\| \\vec{c} \\| }  =  \\frac{\\max_{i} \\left( c_{i} \\right) }{ \\| \\vec{c} \\| } \n",
        "\\end{equation}\n",
        "\n",
        "The ratio of the $S_i$ and $S_i'$ becomes\n",
        "\\begin{equation}\n",
        "F=\\frac{ \\sum _{j}^{}w_{ij}c_{j}}{ \\| \\vec{w_i} \\| \\max_{i} \\left( c_{i} \\right) } \n",
        "\\end{equation}\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoUKmZyYlDkL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def SignalFactor_batch(x, r, W):\n",
        "  \n",
        "    xR = relu_covariance_batch(x,r)\n",
        "    \n",
        "    nm = (tf.reduce_max(tf.abs(xR),axis=0,keepdims=True))\n",
        "    c_hat = xR/nm\n",
        "\n",
        "    \n",
        "    count = tf.reduce_sum(tf.cast(c_hat*W<0, tf.float32))\n",
        "    numer = tf.reduce_sum(c_hat*W, axis = 0)\n",
        "  \n",
        "    norm_wtv = tf.norm(W, axis=0)\n",
        "\n",
        "    denom = norm_wtv\n",
        "\n",
        "    return (numer/denom, count)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07u-Yt6es3Ri",
        "colab_type": "text"
      },
      "source": [
        "# Model definition\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GclIlK14Sz4",
        "colab_type": "text"
      },
      "source": [
        "## Base class\n",
        "Define our base class. The construction is relatively low-level to allow easy access to all the information needed for the figures-of-merit.\n",
        "\n",
        "Includes weights, plus methods for classify and SNR figures-of-merit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2iOnaKaXnsN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class trainable_function:\n",
        "\n",
        "  def __init__(self, withWeightsFrom = None):\n",
        "    self.W = []\n",
        "    self.b = []\n",
        "\n",
        "    node_list = [784, 1024, 1000, 10]\n",
        "\n",
        "    # Set up the weights\n",
        "    if withWeightsFrom == None:\n",
        "      for last_n, nodes in zip(node_list[:-1], node_list[1:]):\n",
        "        self.W.append(weight_variable([last_n, nodes]))\n",
        "        self.b.append(bias_variable([nodes]))\n",
        "    \n",
        "    else:\n",
        "      self.W = [tf.Variable(k) for k in withWeightsFrom.W]\n",
        "      self.b = [tf.Variable(k) for k in withWeightsFrom.b]\n",
        "\n",
        "    # record the trainable variables\n",
        "    self.variables = [k for k in self.W] + [k for k in self.b]\n",
        "\n",
        "    # Build the network\n",
        "    self.layer_dot = [lambda x, p: x]\n",
        "    self.layer_relu = [lambda x, p: tf.reshape(x,[-1,784])]\n",
        "    \n",
        "    for kk in range(1,len(self.W)):\n",
        "      self.layer_dot.append(dot_layer(self.layer_relu[kk-1], self.W[kk-1])) \n",
        "      self.layer_relu.append(relu_layer(self.layer_dot[kk], self.b[kk-1]))\n",
        "\n",
        "    self.layer_dot.append(dot_layer(self.layer_relu[-1], self.W[-1])) \n",
        "\n",
        "    self.optimiser = tf.train.GradientDescentOptimizer(0.1)\n",
        "\n",
        "    \n",
        "  ##\n",
        "  # Required interfaces for the class\n",
        "  def classify(self, x, p = 1.):\n",
        "    return self.layer_dot[-1](x,p)\n",
        "\n",
        "  def train(self, x, y):\n",
        "    None\n",
        "\n",
        "\n",
        "  ##\n",
        "  # SNR instrumentation\n",
        "\n",
        "  # Calculate F for one layer\n",
        "  def SignalFactor(self, x, layer):\n",
        "    x_int = self.layer_relu[layer-1](x,1.)\n",
        "    r = tf.matmul(x_int, self.W[layer-1]) \n",
        "    return SignalFactor_batch(x_int, r, self.W[layer-1])\n",
        "\n",
        "\n",
        "  # Calculate mean(F) for each layer\n",
        "  def SF_summary(self, x):\n",
        "    res = []\n",
        "    for k in range(len(self.layer_relu)):\n",
        "      (measure, count)  = self.SignalFactor(x, k+1)\n",
        "      res.append(masked_mean(measure))\n",
        "    return res\n",
        "\n",
        "\n",
        "  # Calculate S for one layer\n",
        "  def SNROptimality(self, x, layer):\n",
        "    x_int = self.layer_relu[layer-1](x,1.)\n",
        "    r = tf.matmul(x_int, self.W[layer-1]) \n",
        "    return SNROptimality_batch(x_int, r, self.W[layer-1])\n",
        "  \n",
        "  # Calculate mean(S) for each layer\n",
        "  def SNR_summary(self, x):\n",
        "    res = []\n",
        "    for k in range(len(self.layer_relu)):\n",
        "      (measure, count)  = self.SNROptimality(x, k+1)\n",
        "      res.append(masked_mean(measure))\n",
        "    return res\n",
        "\n",
        "\n",
        "  # Plot w against c for set of nodes\n",
        "  def Optimality_plot(self, x, layer, nodes):\n",
        "    x_int = self.layer_relu[layer-1](x,1.)\n",
        "    r = tf.matmul(x_int, self.W[layer-1]) \n",
        "    return Optimality_Plot(x_int, r, self.W[layer-1], nodes)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5kK6EBK4Ky0",
        "colab_type": "text"
      },
      "source": [
        "## Sub-classes \n",
        "\n",
        "Sub-classes representing different training methods/regularisation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXpADLpe32q9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##\n",
        "# Basic SGD with no regulsatisation\n",
        "class trainable_function_with_noreg(trainable_function):\n",
        "\n",
        "  def calc_grad(self, inputs, targets):\n",
        "    with tf.GradientTape() as tape:\n",
        "      loss_value = cross_entropy(self.classify(inputs, p = 1.), targets)\n",
        "    return tape.gradient(loss_value, self.variables)\n",
        "\n",
        "  def train(self, x,y):\n",
        "    grads = self.calc_grad(x, y)\n",
        "    self.optimiser.apply_gradients(zip(grads, self.variables))\n",
        "\n",
        "##\n",
        "# Basic SGD with dropout\n",
        "class trainable_function_with_dropout(trainable_function):\n",
        "\n",
        "  def calc_grad(self, inputs, targets):\n",
        "    with tf.GradientTape() as tape:\n",
        "      loss_value = cross_entropy(self.classify(inputs, p = 0.5), targets)\n",
        "    return tape.gradient(loss_value, self.variables)\n",
        "\n",
        "  def train(self, x,y):\n",
        "    grads = self.calc_grad(x, y)\n",
        "    self.optimiser.apply_gradients(zip(grads, self.variables))\n",
        "    \n",
        "##\n",
        "# Basic SGD with L2 regulsatisation\n",
        "class trainable_function_with_L2(trainable_function):\n",
        "\n",
        "  def calc_grad(self, inputs, targets):\n",
        "    with tf.GradientTape() as tape:\n",
        "      loss_value = cross_entropy(self.classify(inputs, p = 1.), targets) \\\n",
        "                                 +0.00005*(tf.nn.l2_loss(self.W[0])\\\n",
        "                                 +tf.nn.l2_loss(self.W[1])\\\n",
        "                                 +tf.nn.l2_loss(self.W[2]))\n",
        "                                 \n",
        "    return tape.gradient(loss_value, self.variables)\n",
        "\n",
        "  def train(self, x,y):\n",
        "    grads = self.calc_grad(x, y)\n",
        "    self.optimiser.apply_gradients(zip(grads, self.variables))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlThZLN1svDs",
        "colab_type": "text"
      },
      "source": [
        "#Â Training & evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDOPNrG_35a_",
        "colab_type": "text"
      },
      "source": [
        "Set up three models for comparison. We use identical starting weights and training data, to ensure that there is a comparison between training methods, not simply the initialisation and data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYDLOui60Kmg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "control = trainable_function()\n",
        "model_n = trainable_function_with_noreg(control)\n",
        "model_l2 = trainable_function_with_L2(control)\n",
        "model_d = trainable_function_with_dropout(control)\n",
        "\n",
        "model_list = [model_n, model_l2, model_d]\n",
        "\n",
        "test_acc = {lst: [] for lst in model_list} \n",
        "SNR = {lst: [] for lst in model_list} \n",
        "SF = {lst: [] for lst in model_list} \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahbd233Tk7gF",
        "colab_type": "text"
      },
      "source": [
        "Training with regular calculations of the figures-of-merit. A good time for a coffee.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "5371e34b-d147-471f-b2ea-9ebf997ae6bf",
        "id": "MetEhGoi_91A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "source": [
        "# Main loop\n",
        "for i in range(100001):\n",
        "  \n",
        "  dat = data_iterator.get_next()\n",
        "\n",
        "  # Loop over models\n",
        "  for model in model_list:\n",
        "\n",
        "      # Instrumentation\n",
        "\n",
        "      if i % 200 == 0:\n",
        "        test_accuracy  = measure_accuracy(labels = test_labels, logits = model.classify(test_images))\n",
        "        test_acc[model].append(test_accuracy)\n",
        "\n",
        "\n",
        "      if i % 2000 == 0:\n",
        "        SF[model].append(model.SF_summary(test_images))\n",
        "        SNR[model].append(model.SNR_summary(test_images))\n",
        "        \n",
        "\n",
        "      if i % 20000 == 0:\n",
        "        print('\\n Step %d' % i)\n",
        "        print(type(model).__name__)\n",
        "        print('Test accuracy %g' % test_accuracy)\n",
        "        print('L1: %g' % SNR[model][-1][0])\n",
        "        print('L2: %g' % SNR[model][-1][1])\n",
        "        print('L3: %g' % SNR[model][-1][2])\n",
        "\n",
        "\n",
        "      # Actual training          \n",
        "      model.train(dat['image'], dat['label'])\n",
        "\n",
        "  if i % 20000 == 0:\n",
        "    print('------\\n')\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Step 0\n",
            "trainable_function_with_noreg\n",
            "Test accuracy 0.1009\n",
            "L1: 0.146117\n",
            "L2: 0.177978\n",
            "L3: 0.138128\n",
            "\n",
            " Step 0\n",
            "trainable_function_with_L2\n",
            "Test accuracy 0.1009\n",
            "L1: 0.146117\n",
            "L2: 0.177978\n",
            "L3: 0.138128\n",
            "\n",
            " Step 0\n",
            "trainable_function_with_dropout\n",
            "Test accuracy 0.1009\n",
            "L1: 0.146117\n",
            "L2: 0.177978\n",
            "L3: 0.138128\n",
            "------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzVct-pH208w",
        "colab_type": "text"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3fyuw2pyA5F",
        "colab_type": "text"
      },
      "source": [
        "## Test accuracy for each model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WINtJPKuptjG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for m in model_list:\n",
        "  plt.plot(test_acc[m])\n",
        "plt.ylim([0.98,0.99])\n",
        "plt.ylabel('test accuracy')\n",
        "plt.xlabel('iterations (200x)')\n",
        "plt.legend([type(model).__name__ for model in model_list])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0G2OR1MyGMn",
        "colab_type": "text"
      },
      "source": [
        "## SNR optimality\n",
        "\n",
        "Note that the SNR optimality is correlated with performance after convergence. Interestingly, for dropout the SNR optimality indicates the enhanced performance before this is seen in the test accuracy results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kx8DPx0O9lI3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, a = plt.subplots(1,3,figsize=(15, 5))\n",
        "for m in model_list:\n",
        "  a[0].plot([a[0] for a in SNR[m]])\n",
        "  a[1].plot([a[1] for a in SNR[m]])\n",
        "  a[2].plot([a[2] for a in SNR[m]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PO9FUO44er7i",
        "colab_type": "text"
      },
      "source": [
        "Note that the dropout $S$ increases before the test accuracy, giving an early indication of better perfromance. \n",
        "See paper for (brief) discussion of long-term $l_2$ behaviour.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LD78JeA-yb-9",
        "colab_type": "text"
      },
      "source": [
        "## Signal Figure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCZf_HW5Ehvz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, a = plt.subplots(1,3,figsize=(15, 5))\n",
        "for m in model_list:\n",
        "  a[0].plot([a[0] for a in SF[m]])\n",
        "  a[1].plot([a[1] for a in SF[m]])\n",
        "  a[2].plot([a[2] for a in SF[m]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzDLh_XCx2n4",
        "colab_type": "text"
      },
      "source": [
        "## Scatter plots of $W$ vs. $c$\n",
        "\n",
        "Here, we plot $W$ against $c$ for one node. This gives a more intuitive & visual sense of what it means to have good SNR optimality. The points become increasingly close to the line $W=c$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_EUNVcX7wsg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for m in [control]+model_list:\n",
        "  ind = tf.argmax(m.SNROptimality(test_images,3)[0])\n",
        "  ax = m.Optimality_plot(test_images,3,ind)\n",
        "  ax.set_title(type(m).__name__)\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKwNCTSE1H5p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}